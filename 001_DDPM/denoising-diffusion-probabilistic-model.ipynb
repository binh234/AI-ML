{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install libraries","metadata":{}},{"cell_type":"code","source":"# !nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q torch torchvision torchmetrics lightning","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-16T01:13:08.461511Z","iopub.execute_input":"2023-04-16T01:13:08.461957Z","iopub.status.idle":"2023-04-16T01:13:23.379366Z","shell.execute_reply.started":"2023-04-16T01:13:08.461918Z","shell.execute_reply":"2023-04-16T01:13:23.378178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import gc\nimport os\nimport cv2\nimport math\nimport base64\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import Adam, AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.swa_utils import AveragedModel, update_bn\n\nimport torchvision\nimport torchvision.transforms as TF\nimport torchvision.datasets as datasets\nfrom torchvision.utils import make_grid\n\nfrom lightning.pytorch import LightningModule, Trainer\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\nfrom torchmetrics import MeanMetric\n\nfrom IPython.display import display, HTML, clear_output","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:23.381679Z","iopub.execute_input":"2023-04-16T01:13:23.382370Z","iopub.status.idle":"2023-04-16T01:13:31.988520Z","shell.execute_reply.started":"2023-04-16T01:13:23.382326Z","shell.execute_reply":"2023-04-16T01:13:31.987399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions","metadata":{}},{"cell_type":"code","source":"def to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader:\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl:\n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)\n\n# def get_default_device():\n#     return torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef get_num_parameter(model):\n    # Count the number of parameters\n    num_params = sum(p.numel() for p in model.parameters())\n    return num_params\n\ndef save_images(images, path, **kwargs):\n    grid = make_grid(images, **kwargs)\n    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n    im = Image.fromarray(ndarr)\n    im.save(path)\n    \ndef get(element: torch.Tensor, t: torch.Tensor):\n    \"\"\"\n    Get value at index position \"t\" in \"element\" and\n        reshape it to have the same dimension as a batch of images.\n    \"\"\"\n    ele = element.gather(-1, t)\n    return ele.reshape(-1, 1, 1, 1)\n\ndef setup_log_directory(config):\n    '''Log and Model checkpoint directory Setup'''\n    \n    if os.path.isdir(config.root_log_dir):\n        # Get all folders numbers in the root_log_dir\n        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(config.root_log_dir)]\n        \n        # Find the latest version number present in the log_dir\n        last_version_number = max(folder_numbers)\n\n        # New version name\n        version_name = f\"version_{last_version_number + 1}\"\n\n    else:\n        version_name = config.log_dir\n\n    # Update the training config default directory \n    log_dir        = os.path.join(config.root_log_dir,        version_name)\n    checkpoint_dir = os.path.join(config.root_checkpoint_dir, version_name)\n\n    # Create new directory for saving new experiment version\n    os.makedirs(log_dir,        exist_ok=True)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    print(f\"Logging at: {log_dir}\")\n    print(f\"Model Checkpoint at: {checkpoint_dir}\")\n    \n    return log_dir, checkpoint_dir\n\ndef frames2vid(images, save_path):\n\n    WIDTH = images[0].shape[1]\n    HEIGHT = images[0].shape[0]\n\n#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n#     fourcc = 0\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    video = cv2.VideoWriter(save_path, fourcc, 25, (WIDTH, HEIGHT))\n\n    # Appending the images to the video one by one\n    for image in images:\n        video.write(image)\n\n    # Deallocating memories taken for window creation\n    # cv2.destroyAllWindows()\n    video.release()\n    return\n\ndef display_gif(gif_path):\n    b64 = base64.b64encode(open(gif_path, 'rb').read()).decode('ascii')\n    display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))\n\ndef display_image(image):\n    plt.figure(figsize=(12, 6), facecolor='white')\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:31.993410Z","iopub.execute_input":"2023-04-16T01:13:31.995808Z","iopub.status.idle":"2023-04-16T01:13:32.020183Z","shell.execute_reply.started":"2023-04-16T01:13:31.995764Z","shell.execute_reply":"2023-04-16T01:13:32.018051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configurations","metadata":{}},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass BaseConfig:\n#     DEVICE = get_default_device()\n    DATASET = \"Anime-face-128\"  #  \"MNIST\", \"Cifar-10\", \"Cifar-100\", \"Flowers\", \"Animal-10\", \"Animal-90\", \"Anime-face\", \"Anime-face-128\"\n    IMG_SIZE = 64\n \n    # For logging inferece images and saving checkpoints.\n    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n \n    # Current log and checkpoint directory.\n    log_dir = \"version_0\"\n    checkpoint_dir = \"version_0\"\n \n \n@dataclass\nclass TrainingConfig:\n    TIMESTEPS = 1000  # Define number of diffusion timesteps\n    IMG_SHAPE = (1, BaseConfig.IMG_SIZE, BaseConfig.IMG_SIZE) if BaseConfig.DATASET == \"MNIST\" else (3, BaseConfig.IMG_SIZE, BaseConfig.IMG_SIZE)\n    NUM_EPOCHS = 800\n    TRAINING_SAMPLES = 2e6\n    BATCH_SIZE = 32\n    LR = 2e-4\n    NUM_WORKERS = 2\n    PRECISION = '16-mixed'","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.025547Z","iopub.execute_input":"2023-04-16T01:13:32.028594Z","iopub.status.idle":"2023-04-16T01:13:32.039503Z","shell.execute_reply.started":"2023-04-16T01:13:32.028557Z","shell.execute_reply":"2023-04-16T01:13:32.038559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 32x32\n# @dataclass\n# class ModelConfig:\n#     BASE_CH = 64  # 64, 128, 256, 256\n#     BASE_CH_MULT = (1, 2, 4, 4) # 32, 16, 8, 4\n#     APPLY_ATTENTION = (False, True, True, False)\n#     DROPOUT_RATE = 0.1\n#     TIME_EMB_MULT = 4 # 128\n\n# 64x64\n@dataclass\nclass ModelConfig:\n    BASE_CH = 128  # 128, 256, 256, 512\n    BASE_CH_MULT = (1, 2, 2, 4) # 64, 32, 16, 8\n    APPLY_ATTENTION = (False, False, True, True)\n    DROPOUT_RATE = 0.1\n    TIME_EMB_MULT = 4 # 128\n\n# 128x128\n# @dataclass\n# class ModelConfig:\n#     BASE_CH = 128  # 128, 256, 256, 512, 512\n#     BASE_CH_MULT = (1, 2, 2, 4, 4) # 128, 64, 32, 16, 8\n#     APPLY_ATTENTION = (False, False, False, True, False)\n#     DROPOUT_RATE = 0.1\n#     TIME_EMB_MULT = 4 # 128\n\n# 256x256\n# @dataclass\n# class ModelConfig:\n#     BASE_CH = 128  # 128, 256, 256, 512, 512, 512\n#     BASE_CH_MULT = (1, 2, 2, 4, 4, 4) # 256, 128, 64, 32, 16, 8\n#     APPLY_ATTENTION = (False, False, False, False, True, False)\n#     DROPOUT_RATE = 0.1\n#     TIME_EMB_MULT = 4 # 128","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.044162Z","iopub.execute_input":"2023-04-16T01:13:32.047162Z","iopub.status.idle":"2023-04-16T01:13:32.055539Z","shell.execute_reply.started":"2023-04-16T01:13:32.047125Z","shell.execute_reply":"2023-04-16T01:13:32.054618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UNet Model","metadata":{}},{"cell_type":"code","source":"class SinusoidalPositionEmbeddings(nn.Module):\n    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n        super().__init__()\n\n        half_dim = time_emb_dims // 2\n\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n\n        ts = torch.arange(total_time_steps, dtype=torch.float32)\n\n        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n\n        self.time_blocks = nn.Sequential(\n            nn.Embedding.from_pretrained(emb),\n            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n            nn.SiLU(),\n            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n        )\n\n    def forward(self, time):\n        return self.time_blocks(time)\n\nclass AttentionBlock(nn.Module):\n    def __init__(self, channels=64):\n        super().__init__()\n        self.channels = channels\n\n        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n\n    def forward(self, x):\n        B, _, H, W = x.shape\n        h = self.group_norm(x)\n        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n        return x + h\n\nclass ResnetBlock(nn.Module):\n    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.act_fn = nn.SiLU()\n        # Group 1\n        self.normlize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n\n        # Group 2 time embedding\n        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n\n        # Group 3\n        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n        self.dropout = nn.Dropout2d(p=dropout_rate)\n        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n\n        if self.in_channels != self.out_channels:\n            self.match_input = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1)\n        else:\n            self.match_input = nn.Identity()\n\n        if apply_attention:\n            self.attention = AttentionBlock(channels=self.out_channels)\n        else:\n            self.attention = nn.Identity()\n\n    def forward(self, x, t):\n        # group 1\n        h = self.act_fn(self.normlize_1(x))\n        h = self.conv_1(h)\n\n        # group 2\n        # add in timestep embedding\n        h += self.dense_1(self.act_fn(t))[:, :, None, None]\n\n        # group 3\n        h = self.act_fn(self.normlize_2(h))\n        h = self.dropout(h)\n        h = self.conv_2(h)\n\n        # Residual and attention\n        h = h + self.match_input(x)\n        h = self.attention(h)\n        \n        return h\n\nclass DownSample(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x, *args):\n        return self.downsample(x)\n\nclass UpSample(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.upsample = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n            nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=1, padding=\"same\")\n        )\n\n    def forward(self, x, *args):\n        return self.upsample(x)\n\nclass UNet(nn.Module):\n    def __init__(\n        self,\n        input_channels=3,\n        output_channels=3,\n        num_res_blocks=2,\n        base_channels=128,\n        base_channels_multiples=(1, 2, 4, 8),\n        apply_attention=(False, False, True, False),\n        dropout_rate=0.1,\n        time_multiple=4,\n    ):\n        super().__init__()\n\n        time_emb_dims_exp = base_channels * time_multiple\n        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels, time_emb_dims_exp=time_emb_dims_exp)\n\n        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels, kernel_size=3, stride=1, padding=\"same\")\n\n        num_resolutions = len(base_channels_multiples)\n\n        # Encoder part of the UNet. Dimension reduction.\n        self.encoder_blocks = nn.ModuleList()\n        curr_channels = [base_channels]\n        in_channels = base_channels\n\n        for level in range(num_resolutions):\n            out_channels = base_channels * base_channels_multiples[level]\n\n            for _ in range(num_res_blocks):\n\n                block = ResnetBlock(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    dropout_rate=dropout_rate,\n                    time_emb_dims=time_emb_dims_exp,\n                    apply_attention=apply_attention[level],\n                )\n                self.encoder_blocks.append(block)\n                \n                in_channels = out_channels\n                curr_channels.append(in_channels)\n\n            if level != (num_resolutions - 1):\n                self.encoder_blocks.append(DownSample(channels=in_channels))\n                curr_channels.append(in_channels)\n\n        # Bottleneck in between\n        self.bottleneck_blocks = nn.ModuleList(\n            (\n                ResnetBlock(\n                    in_channels=in_channels,\n                    out_channels=in_channels,\n                    dropout_rate=dropout_rate,\n                    time_emb_dims=time_emb_dims_exp,\n                    apply_attention=True,\n                ),\n                ResnetBlock(\n                    in_channels=in_channels,\n                    out_channels=in_channels,\n                    dropout_rate=dropout_rate,\n                    time_emb_dims=time_emb_dims_exp,\n                    apply_attention=False,\n                ),\n            )\n        )\n\n        # Decoder part of the UNet. Dimension restoration with skip-connections.\n        self.decoder_blocks = nn.ModuleList()\n\n        for level in reversed(range(num_resolutions)):\n            out_channels = base_channels * base_channels_multiples[level]\n\n            for _ in range(num_res_blocks + 1):\n                encoder_in_channels = curr_channels.pop()\n                block = ResnetBlock(\n                    in_channels=encoder_in_channels + in_channels,\n                    out_channels=out_channels,\n                    dropout_rate=dropout_rate,\n                    time_emb_dims=time_emb_dims_exp,\n                    apply_attention=apply_attention[level],\n                )\n\n                in_channels = out_channels\n                self.decoder_blocks.append(block)\n\n            if level != 0:\n                self.decoder_blocks.append(UpSample(in_channels))\n\n        self.final = nn.Sequential(\n            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n            nn.SiLU(),\n            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\"),\n        )\n\n    def forward(self, x, t):\n        time_emb = self.time_embeddings(t)\n\n        h = self.first(x)\n        outs = [h]\n\n        for layer in self.encoder_blocks:\n            h = layer(h, time_emb)\n            outs.append(h)\n\n        for layer in self.bottleneck_blocks:\n            h = layer(h, time_emb)\n\n        for layer in self.decoder_blocks:\n            if isinstance(layer, ResnetBlock):\n                out = outs.pop()\n                h = torch.cat([h, out], dim=1)\n            h = layer(h, time_emb)\n\n        h = self.final(h)\n\n        return h","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.060195Z","iopub.execute_input":"2023-04-16T01:13:32.062780Z","iopub.status.idle":"2023-04-16T01:13:32.112813Z","shell.execute_reply.started":"2023-04-16T01:13:32.062744Z","shell.execute_reply":"2023-04-16T01:13:32.111782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Loader","metadata":{}},{"cell_type":"code","source":"def get_dataset(dataset_name='MNIST'):\n    transforms = torchvision.transforms.Compose(\n        [\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Resize((BaseConfig.IMG_SIZE, BaseConfig.IMG_SIZE), \n                                          interpolation=torchvision.transforms.InterpolationMode.BICUBIC, \n                                          antialias=True),\n            torchvision.transforms.RandomHorizontalFlip(),\n#             torchvision.transforms.Normalize(MEAN, STD),\n            torchvision.transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n        ]\n    )\n     \n    if dataset_name.upper() == \"MNIST\":\n        dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms)\n    elif dataset_name == \"Cifar-10\":    \n        dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transforms)\n    elif dataset_name == \"Cifar-100\":\n        dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transforms)\n    elif dataset_name == \"Flowers\":\n        dataset = datasets.ImageFolder(root=\"/kaggle/input/flowers-recognition/flowers\", transform=transforms)\n    elif dataset_name == \"Animal-10\":\n        dataset = datasets.ImageFolder(root=\"/kaggle/input/animals10/raw-img\", transform=transforms)\n    elif dataset_name == \"Animal-90\":\n        dataset = datasets.ImageFolder(root=\"/kaggle/input/animal-image-dataset-90-different-animals/animals/animals\", transform=transforms)\n    elif dataset_name == \"Anime-face\":\n        dataset = datasets.ImageFolder(root=\"/kaggle/input/animefacedataset\", transform=transforms)\n    elif dataset_name == \"Anime-face-128\":\n        dataset = datasets.ImageFolder(root=\"/kaggle/input/anime-faces-dataset\", transform=transforms)\n    \n    else:\n        raise Exception(\"Upsupported dataset\")\n    return dataset\n\ndef inverse_transform(tensors):\n    \"\"\"Convert tensors from [-1, 1] to [0, 255]\"\"\"\n    return ((tensors.clip(-1, 1) + 1.0) / 2.0) * 255.0","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.117366Z","iopub.execute_input":"2023-04-16T01:13:32.119561Z","iopub.status.idle":"2023-04-16T01:13:32.133168Z","shell.execute_reply.started":"2023-04-16T01:13:32.119525Z","shell.execute_reply":"2023-04-16T01:13:32.132257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_dataloader(\n    dataset_name=\"MNIST\",\n    batch_size=32,\n    pin_memory=False,\n    shuffle=True,\n    num_workers=0,\n):\n    dataset = get_dataset(dataset_name)\n    dataloader = DataLoader(\n        dataset, batch_size=batch_size,\n        pin_memory=pin_memory,\n        num_workers=num_workers,\n        persistent_workers=num_workers > 0,\n        shuffle=shuffle)\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.137683Z","iopub.execute_input":"2023-04-16T01:13:32.140807Z","iopub.status.idle":"2023-04-16T01:13:32.148636Z","shell.execute_reply.started":"2023-04-16T01:13:32.140770Z","shell.execute_reply":"2023-04-16T01:13:32.147774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize dataset","metadata":{}},{"cell_type":"code","source":"loader = get_dataloader(\n    dataset_name=BaseConfig.DATASET,\n    batch_size=128,\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:13:32.152833Z","iopub.execute_input":"2023-04-16T01:13:32.155911Z","iopub.status.idle":"2023-04-16T01:14:09.076826Z","shell.execute_reply.started":"2023-04-16T01:13:32.155876Z","shell.execute_reply":"2023-04-16T01:14:09.074713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6), facecolor='white')\n\nfor b_image, _ in loader:\n    b_image = inverse_transform(b_image)\n    grid_img = make_grid(b_image / 255.0, nrow=16, padding=True, pad_value=1, normalize=True)\n    plt.imshow(grid_img.permute(1, 2, 0))\n    plt.axis('off')\n    break","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:09.080937Z","iopub.execute_input":"2023-04-16T01:14:09.081256Z","iopub.status.idle":"2023-04-16T01:14:10.543106Z","shell.execute_reply.started":"2023-04-16T01:14:09.081227Z","shell.execute_reply":"2023-04-16T01:14:10.542232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Diffusion Process","metadata":{}},{"cell_type":"code","source":"class SimpleDiffusion(nn.Module):\n    def __init__(\n        self,\n        num_diffusion_timesteps=1000,\n        img_shape=(3, 64, 64),\n    ):\n        super().__init__()\n        self.num_diffusion_timesteps = num_diffusion_timesteps\n        self.img_shape = img_shape\n                \n        # BETAs & ALPHAs required at different places in the Algorithm.\n        beta = self.get_betas()\n        \n        self.register_buffer('beta', beta)\n        self.register_buffer('alpha', 1 - beta)\n        self.register_buffer('sqrt_beta', torch.sqrt(beta))\n        self.register_buffer('alpha_cumulative', torch.cumprod(self.alpha, dim=0))\n        self.register_buffer('sqrt_alpha_cumulative', torch.sqrt(self.alpha_cumulative))\n        self.register_buffer('one_by_sqrt_alpha', 1. / torch.sqrt(self.alpha))\n        self.register_buffer('sqrt_one_minus_alpha_cumulative', torch.sqrt(1 - self.alpha_cumulative))\n\n        \n    def get_betas(self):\n        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n        scale = 1000 / self.num_diffusion_timesteps\n        beta_start = scale * 1e-4\n        beta_end = scale * 0.02\n        return torch.linspace(\n            beta_start,\n            beta_end,\n            self.num_diffusion_timesteps,\n            dtype=torch.float32,\n        )\n        \n    def forward(self, x0: torch.Tensor, timesteps: torch.Tensor):\n        eps     = torch.randn_like(x0)  # Noise\n        mean    = get(self.sqrt_alpha_cumulative, t=timesteps) * x0  # Image scaled\n        std_dev = get(self.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n        sample  = mean + std_dev * eps # scaled inputs * scaled noise\n\n        return sample, eps  # return ... , gt noise --> model predicts this)\n    \n    def reverse(self, x, ts, z, predicted_noise):\n        beta_t                            = get(self.beta, ts)\n        one_by_sqrt_alpha_t               = get(self.one_by_sqrt_alpha, ts)\n        sqrt_one_minus_alpha_cumulative_t = get(self.sqrt_one_minus_alpha_cumulative, ts) \n\n        x = (\n            one_by_sqrt_alpha_t\n            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n            + torch.sqrt(beta_t) * z\n        )\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:10.544168Z","iopub.execute_input":"2023-04-16T01:14:10.544501Z","iopub.status.idle":"2023-04-16T01:14:10.559750Z","shell.execute_reply.started":"2023-04-16T01:14:10.544468Z","shell.execute_reply":"2023-04-16T01:14:10.558816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forward Diffusion","metadata":{}},{"cell_type":"code","source":"sd = SimpleDiffusion(\n    num_diffusion_timesteps=TrainingConfig.TIMESTEPS\n)\nloader = iter(\n    get_dataloader(\n        dataset_name=BaseConfig.DATASET,\n        batch_size=6,\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:10.561485Z","iopub.execute_input":"2023-04-16T01:14:10.562199Z","iopub.status.idle":"2023-04-16T01:14:19.053071Z","shell.execute_reply.started":"2023-04-16T01:14:10.562162Z","shell.execute_reply":"2023-04-16T01:14:19.052014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x0s, _ = next(loader)\n\nnoisy_images = []\nspecific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n\nfor timestep in specific_timesteps:\n    timestep = torch.as_tensor(timestep, dtype=torch.long)\n    xts, _ = sd(x0s, timestep)\n    xts = make_grid(xts, nrow=1, padding=1)\n    \n    noisy_images.append(xts)\n\n_, ax = plt.subplots(1, len(noisy_images), figsize=(10, 5), facecolor='white')\n\nfor i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n    ax[i].imshow(noisy_sample.squeeze(0).permute(1, 2, 0))\n    ax[i].set_title(f't={timestep}',fontsize=8)\n    ax[i].axis('off')\n    ax[i].grid(False)\n\nplt.suptitle(\"Forward diffusion process\", y=0.9)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:19.054357Z","iopub.execute_input":"2023-04-16T01:14:19.054710Z","iopub.status.idle":"2023-04-16T01:14:19.623017Z","shell.execute_reply.started":"2023-04-16T01:14:19.054674Z","shell.execute_reply":"2023-04-16T01:14:19.620372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Initialize and confiuration","metadata":{}},{"cell_type":"code","source":"dataloader = get_dataloader(\n    dataset_name  = BaseConfig.DATASET,\n    batch_size    = TrainingConfig.BATCH_SIZE,\n    pin_memory    = True,\n    num_workers   = TrainingConfig.NUM_WORKERS,\n)\nvalid_dataloader = get_dataloader(\n    dataset_name  = BaseConfig.DATASET,\n    batch_size    = 8,\n    pin_memory    = False,\n    shuffle       = False,\n    num_workers   = 1,\n)\n\ntotal_epochs = int(math.ceil(TrainingConfig.TRAINING_SAMPLES / len(dataloader) / TrainingConfig.BATCH_SIZE))\ntotal_epochs = min(TrainingConfig.NUM_EPOCHS, total_epochs)\n\nsample_epoch = int(max(5, total_epochs // 20))\nprint(total_epochs, sample_epoch)\n\ngenerate_video = False\next = \".mp4\" if generate_video else \".png\"\nlog_dir, checkpoint_dir = setup_log_directory(config=BaseConfig())","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:19.624615Z","iopub.execute_input":"2023-04-16T01:14:19.625777Z","iopub.status.idle":"2023-04-16T01:14:28.945452Z","shell.execute_reply.started":"2023-04-16T01:14:19.625726Z","shell.execute_reply":"2023-04-16T01:14:28.944361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(os.path.join(log_dir, 'lightning_logs'), exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:28.946992Z","iopub.execute_input":"2023-04-16T01:14:28.947372Z","iopub.status.idle":"2023-04-16T01:14:28.952769Z","shell.execute_reply.started":"2023-04-16T01:14:28.947333Z","shell.execute_reply":"2023-04-16T01:14:28.951485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Algorithm implementation","metadata":{}},{"cell_type":"markdown","source":"![Algorithm1](https://learnopencv.com/wp-content/uploads/2023/02/denoising-diffusion-probabilistic-models_DDPM_trainig_inference_algorithm-1024x247.png)","metadata":{}},{"cell_type":"code","source":"# Algorithm 1: Training\ndef train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800, total_epochs=total_epochs,\n                   base_config=BaseConfig(), training_config=TrainingConfig()):\n    loss_record = MeanMetric()\n    model.train()\n\n    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n        tq.set_description(f\"Train :: Epoch: {epoch}/{total_epochs}\")\n         \n        for x0s, _ in loader:\n            tq.update(1)\n            \n            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device=base_config.DEVICE)\n            xts, gt_noise = sd(x0s, ts)\n\n            with amp.autocast():\n                pred_noise = model(xts, ts)\n                print(xts.shape, pred_noise.shape)\n                loss = loss_fn(gt_noise, pred_noise)\n\n            optimizer.zero_grad(set_to_none=True)\n            scaler.scale(loss).backward()\n\n            # scaler.unscale_(optimizer)\n            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            loss_value = loss.detach().item()\n            loss_record.update(loss_value)\n\n            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n\n        mean_loss = loss_record.compute().item()\n    \n        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n    \n    return mean_loss","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:28.954424Z","iopub.execute_input":"2023-04-16T01:14:28.955144Z","iopub.status.idle":"2023-04-16T01:14:28.966384Z","shell.execute_reply.started":"2023-04-16T01:14:28.955106Z","shell.execute_reply":"2023-04-16T01:14:28.965348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Algorithm 2: Sampling\n\n@torch.no_grad()\ndef reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64), \n                      num_images=5, nrow=8, device=\"cpu\", generate_video=False, **kwargs):\n\n    x = torch.randn((num_images, *img_shape), device=device)\n    model.eval()\n\n    if kwargs.get(\"generate_video\", False):\n        outs = []\n\n    for time_step in tqdm(iterable=reversed(range(1, timesteps)), \n                          total=timesteps-1, dynamic_ncols=False, \n                          desc=\"Sampling :: \", position=0):\n\n        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n\n        predicted_noise = model(x, ts)\n\n        beta_t                            = get(sd.beta, ts)\n        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts) \n\n        x = (\n            one_by_sqrt_alpha_t\n            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n            + torch.sqrt(beta_t) * z\n        )\n\n        if generate_video:\n            x_inv = inverse_transform(x).type(torch.uint8)\n            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n            outs.append(ndarr)\n\n    if generate_video: # Generate and save video of the entire reverse process. \n        frames2vid(outs, kwargs['save_path'])\n        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n\n    else: # Display and save the image at the final timestep of the reverse process. \n        x = inverse_transform(x).type(torch.uint8)\n        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n        pil_image = TF.functional.to_pil_image(grid)\n        if kwargs.get(\"save_path\", None):\n            pil_image.save(kwargs['save_path'], format=kwargs['save_path'].split(\".\")[-1].upper())\n        display_image(grid.permute(1, 2, 0))\n    return grid","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:28.968337Z","iopub.execute_input":"2023-04-16T01:14:28.969110Z","iopub.status.idle":"2023-04-16T01:14:28.982896Z","shell.execute_reply.started":"2023-04-16T01:14:28.969073Z","shell.execute_reply":"2023-04-16T01:14:28.981882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"markdown","source":"### Vanilla","metadata":{}},{"cell_type":"code","source":"\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# model = UNet(\n#     input_channels          = TrainingConfig.IMG_SHAPE[0],\n#     output_channels         = TrainingConfig.IMG_SHAPE[0],\n#     base_channels           = ModelConfig.BASE_CH,\n#     base_channels_multiples = ModelConfig.BASE_CH_MULT,\n#     apply_attention         = ModelConfig.APPLY_ATTENTION,\n#     dropout_rate            = ModelConfig.DROPOUT_RATE,\n#     time_multiple           = ModelConfig.TIME_EMB_MULT,\n# ).to(device)\n\n# dataloader = DeviceDataLoader(dataloader, device=device)\n\n# optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n\n# loss_fn = nn.MSELoss()\n\n# sd = SimpleDiffusion(\n#     num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n#     img_shape               = TrainingConfig.IMG_SHAPE,\n# ).to(device)\n\n# scaler = torch.cuda.amp.GradScaler()","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:28.984461Z","iopub.execute_input":"2023-04-16T01:14:28.984806Z","iopub.status.idle":"2023-04-16T01:14:28.996550Z","shell.execute_reply.started":"2023-04-16T01:14:28.984772Z","shell.execute_reply":"2023-04-16T01:14:28.995417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for epoch in range(1, total_epochs):\n#     torch.cuda.empty_cache()\n#     gc.collect()\n    \n#     # Algorithm 1: Training\n#     train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n\n#     if epoch % sample_epoch == 0:\n#         save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n        \n#         # Algorithm 2: Sampling\n#         reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=32, generate_video=generate_video,\n#             save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n#         )\n\n#         # clear_output()\n#         checkpoint_dict = {\n#             \"opt\": optimizer.state_dict(),\n#             \"scaler\": scaler.state_dict(),\n#             \"model\": model.state_dict()\n#         }\n#         torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"model.ckpt\"))\n#         del checkpoint_dict","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:28.999763Z","iopub.execute_input":"2023-04-16T01:14:29.000034Z","iopub.status.idle":"2023-04-16T01:14:29.011297Z","shell.execute_reply.started":"2023-04-16T01:14:29.000010Z","shell.execute_reply":"2023-04-16T01:14:29.010233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Pytorch Lightning","metadata":{}},{"cell_type":"code","source":"# Pytorch Lightning\n\nclass LitUNet(LightningModule):\n    def __init__(self, base_config=BaseConfig(), training_config=TrainingConfig(), model_config=ModelConfig()):\n        super().__init__()\n\n        self.save_hyperparameters()\n        self.model = UNet(\n            input_channels          = training_config.IMG_SHAPE[0],\n            output_channels         = training_config.IMG_SHAPE[0],\n            base_channels           = model_config.BASE_CH,\n            base_channels_multiples = model_config.BASE_CH_MULT,\n            apply_attention         = model_config.APPLY_ATTENTION,\n            dropout_rate            = model_config.DROPOUT_RATE,\n            time_multiple           = model_config.TIME_EMB_MULT,\n        )\n        self.sd = SimpleDiffusion(\n            num_diffusion_timesteps = training_config.TIMESTEPS,\n            img_shape               = training_config.IMG_SHAPE,\n        )\n        self.loss_fn = nn.MSELoss()\n        self.generate_video = False\n        self.ext = \".mp4\" if self.generate_video else \".png\"\n    \n    def _calculate_loss(self, batch, mode=\"train\"):\n        x0s, _ = batch\n        ts = torch.randint(low=1, high=self.hparams.training_config.TIMESTEPS, \n                           size=(x0s.shape[0],), device=self.device)\n        xts, gt_noise = self.sd(x0s, ts)\n\n        pred_noise = self.model(xts, ts)\n        loss = self.loss_fn(gt_noise, pred_noise)\n\n        self.log(\"%s_loss\" % mode, loss)\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._calculate_loss(batch, mode=\"train\")\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._calculate_loss(batch, mode=\"val\")\n    \n    def on_validation_epoch_end(self):\n        if self.current_epoch > 0:\n            save_path = os.path.join(self.logger.log_dir, f\"{self.current_epoch}{self.ext}\")\n\n            # Algorithm 2: Sampling\n            grid = reverse_diffusion(\n                self.model, self.sd, timesteps=self.hparams.training_config.TIMESTEPS, num_images=32, generate_video=self.generate_video,\n                save_path=save_path, img_shape=self.hparams.training_config.IMG_SHAPE, device=self.device,\n            )\n            self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=TrainingConfig.LR)\n        return {\"optimizer\": optimizer}","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:29.013138Z","iopub.execute_input":"2023-04-16T01:14:29.013500Z","iopub.status.idle":"2023-04-16T01:14:29.027658Z","shell.execute_reply.started":"2023-04-16T01:14:29.013465Z","shell.execute_reply":"2023-04-16T01:14:29.026696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(dirpath=checkpoint_dir, save_last=True)\nmodel = LitUNet()\ntrainer = Trainer(\n    default_root_dir=log_dir,\n    max_epochs=total_epochs,\n    limit_val_batches=1,\n    check_val_every_n_epoch=sample_epoch,\n    accelerator=\"auto\",\n    devices=\"auto\",  # limiting got iPython runs\n    strategy=\"auto\",\n    callbacks=[checkpoint_callback],\n    log_every_n_steps=20\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:29.029110Z","iopub.execute_input":"2023-04-16T01:14:29.029719Z","iopub.status.idle":"2023-04-16T01:14:29.819767Z","shell.execute_reply.started":"2023-04-16T01:14:29.029682Z","shell.execute_reply":"2023-04-16T01:14:29.818852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(\"Num parameters:\", get_num_parameter(model.model))","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:52.317470Z","iopub.execute_input":"2023-04-16T01:14:52.318441Z","iopub.status.idle":"2023-04-16T01:14:52.323365Z","shell.execute_reply.started":"2023-04-16T01:14:52.318388Z","shell.execute_reply":"2023-04-16T01:14:52.322302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model, dataloader, valid_dataloader)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:14:52.844512Z","iopub.execute_input":"2023-04-16T01:14:52.845215Z","iopub.status.idle":"2023-04-16T01:19:25.977418Z","shell.execute_reply.started":"2023-04-16T01:14:52.845178Z","shell.execute_reply":"2023-04-16T01:19:25.975826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_callback.last_model_path","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:19:28.471625Z","iopub.execute_input":"2023-04-16T01:19:28.471999Z","iopub.status.idle":"2023-04-16T01:19:28.479538Z","shell.execute_reply.started":"2023-04-16T01:19:28.471965Z","shell.execute_reply":"2023-04-16T01:19:28.478432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"IMG_EXTENSIONS = set(['png', 'jpg', 'jpeg'])\nVIDEO_EXTENSIONS = set(['mp4', 'gif'])\n\ndef generate(model, sd, num_images=128, timesteps=1000, nrow=16, format='png', device=\"cpu\"):\n    if format in IMG_EXTENSIONS:\n        generate_video = False\n    elif format in VIDEO_EXTENSIONS:\n        generate_video = True\n    else:\n        print('Unsupported format')\n        return\n\n    filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}.{format}\"\n\n    save_path = os.path.join(log_dir, filename)\n    reverse_diffusion(\n        model,\n        sd,\n        num_images=num_images,\n        generate_video=generate_video,\n        save_path=save_path,\n        timesteps=timesteps,\n        img_shape=TrainingConfig.IMG_SHAPE,\n        nrow=nrow,\n        device=device\n    )\n    print(save_path)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:19:37.006908Z","iopub.execute_input":"2023-04-16T01:19:37.007276Z","iopub.status.idle":"2023-04-16T01:19:37.018162Z","shell.execute_reply.started":"2023-04-16T01:19:37.007244Z","shell.execute_reply":"2023-04-16T01:19:37.017047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# model = UNet(\n#     input_channels          = TrainingConfig.IMG_SHAPE[0],\n#     output_channels         = TrainingConfig.IMG_SHAPE[0],\n#     base_channels           = ModelConfig.BASE_CH,\n#     base_channels_multiples = ModelConfig.BASE_CH_MULT,\n#     apply_attention         = ModelConfig.APPLY_ATTENTION,\n#     dropout_rate            = ModelConfig.DROPOUT_RATE,\n#     time_multiple           = ModelConfig.TIME_EMB_MULT,\n# )\n# model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model']).to(device)\n# sd = SimpleDiffusion(\n#     num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n#     img_shape               = TrainingConfig.IMG_SHAPE,\n# ).to(device)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ncheckpoint_path = os.path.join(checkpoint_dir, 'last.ckpt')\nmodule = LitUNet.load_from_checkpoint(checkpoint_path).to(device)\nmodel = module.model\nsd = module.sd\n\nlog_dir = \"inference_results\"\nos.makedirs(log_dir, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:19:39.064529Z","iopub.execute_input":"2023-04-16T01:19:39.065019Z","iopub.status.idle":"2023-04-16T01:19:41.170256Z","shell.execute_reply.started":"2023-04-16T01:19:39.064974Z","shell.execute_reply":"2023-04-16T01:19:41.169077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(model, sd, format='png', device=device)","metadata":{"execution":{"iopub.status.busy":"2023-04-16T01:19:42.487970Z","iopub.execute_input":"2023-04-16T01:19:42.488391Z","iopub.status.idle":"2023-04-16T01:19:56.443833Z","shell.execute_reply.started":"2023-04-16T01:19:42.488356Z","shell.execute_reply":"2023-04-16T01:19:56.442267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(model, sd, format='png', device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(model, sd, format='png', device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate(model, sd, format='mp4', device=device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n1. [An In-Depth Guide to Denoising Diffusion Probabilistic Models – From Theory to Implementation](https://learnopencv.com/denoising-diffusion-probabilistic-models/#modified-forward-diffusion-kernel)\n2. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239)","metadata":{}}]}